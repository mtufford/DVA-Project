{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import datetime\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve, validation_curve, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "from joblib import dump, load\n",
    "#from process_query import convert_causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_causes(df):\n",
    "    # convert cause in dataframe and return updated dataframe\n",
    "\n",
    "    dict_cause = {\n",
    "        'Lightning': 'Natural',\n",
    "        'Structure': 'Infrastructure Accident',\n",
    "        'Powerline': 'Infrastructure Accident',\n",
    "        'Railroad': 'Infrastructure Accident',\n",
    "        'Fireworks': 'Human Accident',\n",
    "        'Smoking': 'Human Accident',\n",
    "        'Children': 'Human Accident',\n",
    "        'Campfire': 'Human Accident',\n",
    "        'Equipment Use': 'Human Accident',\n",
    "        'Debris Burning': 'Human Accident',\n",
    "        'Arson': 'Arson',\n",
    "        'Missing/Undefined': 'Other',\n",
    "        'Miscellaneous': 'Other'\n",
    "    }\n",
    "\n",
    "    # replace values in cause column if present\n",
    "    if 'STAT_CAUSE_DESCR' in df.columns:\n",
    "        df['STAT_CAUSE_DESCR'].replace(dict_cause, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc_get_data():\n",
    "    # load dataset\n",
    "    try:\n",
    "        conn = sqlite3.connect(r'../FPA_FOD_20170508.sqlite')\n",
    "        sql_query = \\\n",
    "        \"\"\"\n",
    "        SELECT FOD_ID, FIRE_NAME, FIRE_SIZE, FIRE_SIZE_CLASS, LATITUDE, LONGITUDE, STATE, STAT_CAUSE_DESCR, date(DISCOVERY_DATE) AS DATE, FIRE_YEAR FROM Fires;\n",
    "        \"\"\"\n",
    "        data = pd.read_sql(sql_query, conn)\n",
    "\n",
    "        # convert causes\n",
    "        data = convert_causes(data)\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # add columns\n",
    "    data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "    data['MONTH'] = data['DATE'].dt.month\n",
    "    data['DAY_OF_WEEK'] = data['DATE'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "    # drop missing rows\n",
    "    data = data.dropna()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE MACHINE LEARNING MODEL\n",
    "class MLBase:\n",
    "    def __init__(self):\n",
    "        self.estimator = None\n",
    "        self.file_name = None\n",
    "        self.feature_cols = ['LATITUDE', 'LONGITUDE', 'FIRE_YEAR', 'MONTH', 'DAY_OF_WEEK']\n",
    "        self.data = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        \n",
    "        # load dataset\n",
    "        try:\n",
    "            conn = sqlite3.connect(r'../FPA_FOD_20170508.sqlite')\n",
    "            sql_query = \\\n",
    "            \"\"\"\n",
    "            SELECT FOD_ID, FIRE_NAME, FIRE_SIZE, FIRE_SIZE_CLASS, LATITUDE, LONGITUDE, STATE, STAT_CAUSE_DESCR, date(DISCOVERY_DATE) AS DATE, FIRE_YEAR FROM Fires;\n",
    "            \"\"\"\n",
    "            data = pd.read_sql(sql_query, conn)\n",
    "\n",
    "            # convert causes\n",
    "            data = convert_causes(data)\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        # add columns\n",
    "        data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "        data['MONTH'] = data['DATE'].dt.month\n",
    "        data['DAY_OF_WEEK'] = data['DATE'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "        # drop missing rows\n",
    "        self.data = data.dropna()\n",
    "    \n",
    "    def fit_model(self, X, y):\n",
    "        # split dataset into training set and test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "        # fit random forest classifier\n",
    "        self.estimator.fit(X_train, y_train)\n",
    "        \n",
    "        # predict response for test dataset\n",
    "        y_pred = self.estimator.predict(X_test)\n",
    "        \n",
    "        # store accuracy\n",
    "        self.estimator.accuracy_ = metrics.accuracy_score(y_test, y_pred)\n",
    "      \n",
    "    def fit_save(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        self.save()\n",
    "        \n",
    "    def save(self):\n",
    "        # save model to lib folder; to be run within lib directory\n",
    "        print('Compressing and saving model...\\n')\n",
    "        \n",
    "        if self.estimator is not None:\n",
    "            dump(self.estimator, './' + self.file_name, compress = 9)\n",
    "            print('{} has been saved.\\n'.format(self.file_name))\n",
    "        else: \n",
    "            print('Error: No estimator created yet.')\n",
    "    \n",
    "    def load(self):\n",
    "        # load model from lib folder; to be run within app.py\n",
    "        print('Loading {}...\\n'.format(self.file_name))\n",
    "        self.estimator = load('./lib/' + self.file_name)    # access from app.py\n",
    "        print('{} has been loaded.\\n'.format(self.file_name))\n",
    "    \n",
    "    def predict(self, lat, long, year, month, day_of_week):\n",
    "        # predict given LATITUDE, LONGITUDE, FIRE_YEAR, MONTH, DAY_OF_WEEK\n",
    "        if self.estimator is not None:\n",
    "            prediction = self.estimator.predict([[lat, long, year, month, day_of_week]])\n",
    "            return prediction[0]\n",
    "        else: \n",
    "            print('Error: No estimator created yet.')\n",
    "\n",
    "\n",
    "# RANDOM FOREST TO PREDICT WILDFIRE CAUSE\n",
    "class MLRandomForestCause(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = RandomForestClassifier(n_estimators=25)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train random forest with 25 trees; to be run within lib directory\n",
    "        print('Training random forest...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['STAT_CAUSE_DESCR']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('Random forest trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# RANDOM FOREST TO PREDICT WILDFIRE SIZE CLASS\n",
    "class MLRandomForestSizeClass(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = RandomForestClassifier(n_estimators=25)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train random forest with 25 trees; to be run within lib directory\n",
    "        print('Training random forest...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['FIRE_SIZE_CLASS']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('Random forest trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# KNN TO PREDICT WILDFIRE CAUSE\n",
    "class MLKnnCause(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = KNeighborsClassifier(n_neighbors=100)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train knn with 100 neighbors; to be run within lib directory\n",
    "        print('Training KNN...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['STAT_CAUSE_DESCR']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('KNN trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# KNN TO PREDICT WILDFIRE SIZE CLASS\n",
    "class MLKnnSizeClass(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = KNeighborsClassifier(n_neighbors=100)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train knn with 100 neighbors; to be run within lib directory\n",
    "        print('Training KNN...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['FIRE_SIZE_CLASS']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('KNN trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# AdaBoost TO PREDICT WILDFIRE CAUSE\n",
    "class MLAdaBoostCause(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = AdaBoostClassifier(n_estimators=60)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training AdaBoost...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['STAT_CAUSE_DESCR']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('AdaBoost trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# AdaBoost TO PREDICT WILDFIRE SIZE CLASS\n",
    "class MLAdaBoostSizeClass(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = AdaBoostClassifier(n_estimators=60)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training AdaBoost...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['FIRE_SIZE_CLASS']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('AdaBoost trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# Gradient Boosting TO PREDICT WILDFIRE CAUSE\n",
    "class MLGradientBoostingCause(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = GradientBoostingClassifier(max_depth=8, n_estimators=250)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training Gradient Boosting...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['STAT_CAUSE_DESCR']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('Gradient Boosting trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# Gradient Boosting TO PREDICT WILDFIRE SIZE CLASS\n",
    "class MLGradientBoostingClass(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = GradientBoostingClassifier(max_depth=8, n_estimators=250)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training Gradient Boosting...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['FIRE_SIZE_CLASS']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('Gradient Boosting trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "          \n",
    "\n",
    "# SVM TO PREDICT WILDFIRE CAUSE\n",
    "class MLSVMCause(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = SVC(kernel='rbf', C=1.0)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training SVM...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['STAT_CAUSE_DESCR']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('SVM trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))\n",
    "\n",
    "\n",
    "# SVM TO PREDICT WILDFIRE SIZE CLASS\n",
    "class MLSVMSizeClass(MLBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.estimator = SVC(kernel='rbf', C=1.0)\n",
    "        self.file_name = __class__.__name__ + '.joblib.z'  # name of stored class\n",
    "\n",
    "    def train(self, save=False):\n",
    "        # train adaboost; to be run within lib directory\n",
    "        print('Training SVM...\\n')\n",
    "\n",
    "        # load and clean data\n",
    "        self.get_data()\n",
    "\n",
    "        # identify features and label\n",
    "        X = self.data[self.feature_cols]\n",
    "        y = self.data['FIRE_SIZE_CLASS']\n",
    "\n",
    "        # fit model\n",
    "        if save:\n",
    "          self.fit_save(X, y)\n",
    "        else:\n",
    "          self.fit_model(X, y)\n",
    "\n",
    "          print('SVM trained with accuracy of {}.\\n'.format(self.estimator.accuracy_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=[0.001, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], save=True):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1_weighted')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    # Leveraged from aschang3 ML project\n",
    "    best_index = 0\n",
    "    for i in range(1,len(test_scores_mean)):\n",
    "        if test_scores_mean[i] - (train_scores_mean[i] - test_scores_mean[i])  > test_scores_mean[best_index] - (train_scores_mean[best_index] - test_scores_mean[best_index]):\n",
    "            best_index = i\n",
    "\n",
    "    best_score = test_scores_mean[best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    plt.plot([train_sizes[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color='k', marker='x', markeredgewidth=4, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    plt.annotate(\"%0.3f\" % best_score, (train_sizes[best_index] - 0.1, best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10.5, 10.5)\n",
    "    save_as = '../figures/' + title + \"_learning_curve_\" + datetime.datetime.today().strftime('%d%H%M%S') + '.png'\n",
    "    if save:\n",
    "      fig.savefig(save_as, dpi=100)\n",
    "    plt.close()\n",
    "    # End leveraged from aschang3 ML project\n",
    "\n",
    "    return\n",
    "## reference from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "\n",
    "## reference from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py\n",
    "def plot_model_complexity(estimator, title, X, y, ylim=None, cv=None, param_name=None, param_range=[], save=True):\n",
    "    train_scores, test_scores = validation_curve(estimator, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring='f1_weighted', n_jobs=-1) \n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    lw = 2\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "\n",
    "    # Leveraged from aschang3 ML project\n",
    "    best_index = 0\n",
    "    for i in range(1,len(test_scores_mean)):\n",
    "        if test_scores_mean[i] - (train_scores_mean[i] - test_scores_mean[i])  > test_scores_mean[best_index] - (train_scores_mean[best_index] - test_scores_mean[best_index]):\n",
    "            best_index = i\n",
    "\n",
    "    best_score = test_scores_mean[best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    plt.plot([param_range[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color='k', marker='x', markeredgewidth=4, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    plt.annotate(\"%0.3f\" % best_score, (param_range[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10.5, 10.5)\n",
    "    save_as = '../figures/' + title + datetime.datetime.today().strftime('%d%H%M%S') + '.png'\n",
    "    if save:\n",
    "      fig.savefig(save_as, dpi=100)\n",
    "    plt.close()\n",
    "    # End leveraged from aschang3 ML project\n",
    "\n",
    "    return\n",
    "# reference from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fnc_get_data()\n",
    "feature_cols = ['LATITUDE', 'LONGITUDE', 'FIRE_YEAR', 'MONTH', 'DAY_OF_WEEK']\n",
    "X = df[feature_cols]\n",
    "y_cause = df['STAT_CAUSE_DESCR']\n",
    "y_size = df['FIRE_SIZE_CLASS']\n",
    "cv = ShuffleSplit(n_splits=10, train_size=0.15, test_size=0.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # rf gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'n_estimators': [125,250], 'max_depth':[16,32]}\n",
    "        rforest = RandomForestClassifier()\n",
    "        clf = GridSearchCV(rforest, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])\n",
    "\n",
    "    ## cause\n",
    "    #{'max_depth': 32, 'n_estimators': 250}\n",
    "    #0.6390040225861129\n",
    "    #[{'max_depth': 16, 'n_estimators': 125}, {'max_depth': 16, 'n_estimators': 250}, {'max_depth': 32, 'n_estimators': 125}, {'max_depth': 32, 'n_estimators': 250}]\n",
    "    #[0.63151787 0.63191176 0.63772845 0.63900402]\n",
    "    #[31.2246248  62.86037166 37.67768614 71.36209855]\n",
    "\n",
    "    ## size\n",
    "    #{'max_depth': 16, 'n_estimators': 250}\n",
    "    #0.5742286401718223\n",
    "    #[{'max_depth': 16, 'n_estimators': 125}, {'max_depth': 16, 'n_estimators': 250}, {'max_depth': 32, 'n_estimators': 125}, {'max_depth': 32, 'n_estimators': 250}]\n",
    "    #[0.57404182 0.57422864 0.57088753 0.57173373]\n",
    "    #[39.71029224 75.89663596 52.66529212 85.22966387]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # rf gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'n_estimators': [25,50]}\n",
    "        rforest = RandomForestClassifier()\n",
    "        clf = GridSearchCV(rforest, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])\n",
    "\n",
    "    ## cause\n",
    "    #{'n_estimators': 50}\n",
    "    #0.6345049032411676\n",
    "    #[{'n_estimators': 25}, {'n_estimators': 50}]\n",
    "    #[0.63021669 0.6345049 ]\n",
    "    #[ 8.67082019 17.25797555]\n",
    "\n",
    "    # size\n",
    "    #{'n_estimators': 50}\n",
    "    #0.5680015043876222\n",
    "    #[{'n_estimators': 25}, {'n_estimators': 50}]\n",
    "    #[0.56341574 0.5680015 ]\n",
    "    #[10.82792463 22.12725415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # knn gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'n_neighbors': [5,10,100]}\n",
    "        knn = KNeighborsClassifier()\n",
    "        clf = GridSearchCV(knn, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])\n",
    "\n",
    "    ## cause\n",
    "    #{'n_neighbors': 10}\n",
    "    #0.5905766174878578\n",
    "    #[{'n_neighbors': 5}, {'n_neighbors': 10}, {'n_neighbors': 100}]\n",
    "    #[0.58365336 0.59057662 0.56308848]\n",
    "    #[0.42246768 0.44235063 0.46920006]\n",
    "\n",
    "    ## size\n",
    "    #{'n_neighbors': 100}\n",
    "    #0.5473554376900897\n",
    "    #[{'n_neighbors': 5}, {'n_neighbors': 10}, {'n_neighbors': 100}]\n",
    "    #[0.5309177  0.54406385 0.54735544]\n",
    "    #[0.42958615 0.48844428 0.44010985]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # adaptive boosting gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'n_estimators':[60,250,500,1000]}\n",
    "        aboost = AdaBoostClassifier()\n",
    "        clf = GridSearchCV(aboost, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])\n",
    "\n",
    "    ## cause\n",
    "    #{'n_estimators': 1000}\n",
    "    #0.49302307229299175\n",
    "    #[{'n_estimators': 60}, {'n_estimators': 250}, {'n_estimators': 500}, {'n_estimators': 1000}]\n",
    "    #[0.44783861 0.47919785 0.4884043  0.49302307]\n",
    "    #[ 18.51594372  74.46232986 152.80710304 305.32525001]\n",
    "\n",
    "    ## size\n",
    "    #{'n_estimators': 1000}\n",
    "    #0.5237277189495368\n",
    "    #[{'n_estimators': 60}, {'n_estimators': 250}, {'n_estimators': 500}, {'n_estimators': 1000}]\n",
    "    #[0.5198465  0.52175404 0.52257134 0.52372772]\n",
    "    #[ 13.92950158  60.43023548 132.85321066 245.61578233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # gradient boosting gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'max_depth':[8,12,16]}\n",
    "        gboost = GradientBoostingClassifier()\n",
    "        clf = GridSearchCV(gboost, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])\n",
    "        \n",
    "        ##cause\n",
    "        #{'max_depth': 12}\n",
    "        #0.641225636144493\n",
    "        #[{'max_depth': 8}, {'max_depth': 12}, {'max_depth': 16}]\n",
    "        #[0.62882442 0.64122564 0.63908153]\n",
    "        #[ 317.25169902 1103.92032697 4077.71273634]\n",
    "        \n",
    "        ## size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # histogram gradient boosting gridsearchcv\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'max_depth':[8,12,16], 'learning_rate':[0.05,0.1,0.5,1.0]}\n",
    "        hgboost = HistGradientBoostingClassifier()\n",
    "        clf = GridSearchCV(hgboost, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # svm gridsearchcv\n",
    "    # StandardScaler w/ Pandas DF Reference: https://stackoverflow.com/questions/35723472/how-to-use-sklearn-fit-transform-with-pandas-and-return-dataframe-instead-of-num\n",
    "    X_scaled = preprocessing.StandardScaler().fit_transform(X.values)\n",
    "    for y in [y_cause, y_size]:\n",
    "        parameters = {'C':[0.1,0.5,1.0,1.5,2.5,5.0]}\n",
    "        svc = SVC(kernel='rbf', gamma=\"scale\")\n",
    "        clf = GridSearchCV(svc, parameters, cv=cv, scoring='f1_weighted', verbose=10, n_jobs=4)\n",
    "        clf.fit(X_scaled, y)\n",
    "        print(clf.best_params_)\n",
    "        print(clf.best_score_)\n",
    "        print(clf.cv_results_['params'])\n",
    "        print(clf.cv_results_['mean_test_score'])\n",
    "        print(clf.cv_results_['mean_fit_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11503494 0.39921324 0.02245482 0.23564785 0.22764915]\n",
      "[0.42816449 0.43247631 0.09598538 0.01989438 0.01191193 0.00758603\n",
      " 0.00398147]\n"
     ]
    }
   ],
   "source": [
    "new_array, num_labels = np.unique(y_cause.values, return_counts=True)\n",
    "label_distro = num_labels / len(y_cause)\n",
    "print(label_distro)\n",
    "new_array, num_labels = np.unique(y_size.values, return_counts=True)\n",
    "label_distro = num_labels / len(y_size)\n",
    "print(label_distro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    cv = ShuffleSplit(n_splits=10, train_size=0.15, test_size=0.05, random_state=0)\n",
    "    #rfc\n",
    "    #plot_model_complexity(RandomForestClassifier(n_estimators=125), \"Random Forest Cause Classifier Model Complexity Curve\", X, y_cause, param_name='max_depth', param_range=[2,4,6,8,10,12,16], ylim=[0.3,0.8], cv=cv)\n",
    "    #plot_model_complexity(RandomForestClassifier(n_estimators=125), \"Random Forest Size Classifier Model Complexity Curve\", X, y_size, param_name='max_depth', param_range=[2,4,6,8,10,12,16], ylim=[0.3,0.8], cv=cv)\n",
    "    #knn\n",
    "    #plot_model_complexity(KNeighborsClassifier(), \"KNN Cause Classifier Model Complexity Curve\", X, y_cause, param_name='n_neighbors', param_range=[1,3,5,7,10,15,25], ylim=[0.3,0.8], cv=cv)\n",
    "    #plot_model_complexity(KNeighborsClassifier(), \"KNN Size Classifier Model Complexity Curve\", X, y_size, param_name='n_neighbors', param_range=[1,3,5,7,10,15,25], ylim=[0.3,0.8], cv=cv)\n",
    "    #adaboost\n",
    "    #plot_model_complexity(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=7)), \"AdaBoost Cause Classifier Model Complexity Curve Max Depth 7\", X, y_cause, param_name='n_estimators', param_range=[3,5,10,18,25], ylim=[0.3,0.8], cv=cv)\n",
    "    #plot_model_complexity(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=7)), \"AdaBoost Size Classifier Model Complexity Curve Max Depth 7\", X, y_size, param_name='n_estimators', param_range=[3,5,10,18,25], ylim=[0.3,0.8], cv=cv)\n",
    "    #hist gradient boosting\n",
    "    #plot_model_complexity(HistGradientBoostingClassifier(), \"Histogram Gradient Boosting Cause Classifier Model Complexity Curve\", X, y_cause, param_name='max_depth', param_range=[2,4,8,10,12], ylim=[0.3,0.8], cv=cv)\n",
    "    #plot_model_complexity(HistGradientBoostingClassifier(), \"Histogram Gradient Boosting Size Classifier Model Complexity Curve\", X, y_size, param_name='max_depth', param_range=[2,4,8,10,12], ylim=[0.3,0.8], cv=cv)\n",
    "    #svm\n",
    "    X_scaled = preprocessing.StandardScaler().fit_transform(X.values)\n",
    "    plot_model_complexity(SVC(kernel='rbf', gamma='scale', max_iter=4000), \"SVM Cause Classifier Model Complexity Curve\", X_scaled, y_cause, param_name='C', param_range=[0.1,0.5,1.0,1.5,2.5,5.0], ylim=[0.3,0.8], cv=cv)\n",
    "    #plot_model_complexity(SVC(kernel='rbf', gamma='scale', max_iter=1500), \"SVM Size Classifier Model Complexity Curve\", X_scaled, y_size, param_name='C', param_range=[0.1,0.5,1.0,1.5,2.5,5.0], ylim=[0.3,0.8], cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    cv = ShuffleSplit(n_splits=3, train_size=0.8, test_size=0.2, random_state=0)\n",
    "    #rfc\n",
    "    #plot_learning_curve(RandomForestClassifier(n_estimators=125, max_depth=10), \"Random Forest Cause Classifier Learning Curve\", X, y_cause, ylim=[0.3, 0.8], cv=cv)\n",
    "    #plot_learning_curve(RandomForestClassifier(n_estimators=125, max_depth=10), \"Random Forest Size Classifier Learning Curve\", X, y_size, ylim=[0.3, 0.8], cv=cv)\n",
    "    #knn\n",
    "    #plot_learning_curve(KNeighborsClassifier(n_neighbors=10), \"KNN Cause Classifier Learning Curve\", X, y_cause, ylim=[0.3, 0.8], cv=cv)\n",
    "    #plot_learning_curve(KNeighborsClassifier(n_neighbors=10), \"KNN Size Classifier Learning Curve\", X, y_size, ylim=[0.3, 0.8], cv=cv)\n",
    "    #adaboost\n",
    "    #plot_learning_curve(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=7), n_estimators=10), \"AdaBoost Cause Classifier Learning Curve Max Depth 7\", X, y_cause, ylim=[0.3, 0.8], cv=cv)\n",
    "    #plot_learning_curve(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=7), n_estimators=10), \"AdaBoost Size Classifier Learning Curve Max Depth 7\", X, y_size, ylim=[0.3, 0.8], cv=cv)\n",
    "    #histogram gradient boosting\n",
    "    #plot_learning_curve(HistGradientBoostingClassifier(max_depth=8), \"Histogram Gradient Boosting Cause Classifier Learning Curve\", X, y_cause, ylim=[0.3, 0.8], cv=cv)\n",
    "    #plot_learning_curve(HistGradientBoostingClassifier(max_depth=8), \"Histogram Gradient Boosting Size Classifier Learning Curve\", X, y_size, ylim=[0.3, 0.8], cv=cv)\n",
    "    #svm\n",
    "    X_scaled = preprocessing.StandardScaler().fit_transform(X.values)\n",
    "    plot_learning_curve(SVC(kernel='rbf', gamma='scale', C=1.0, max_iter=1500), \"SVM Cause Classifier Learning Curve\", X_scaled, y_cause, ylim=[0.3, 0.8], cv=cv)\n",
    "    plot_learning_curve(SVC(kernel='rbf', gamma='scale', C=1.0, max_iter=1500), \"SVM Size Classifier Learning Curve\", X_scaled, y_size, ylim=[0.3, 0.8], cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
